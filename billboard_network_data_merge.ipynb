{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c4520b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_artists shape: (14226, 54)\n",
      "df_songs shape: (5099, 16)\n",
      "df_albums shape: (4569, 16)\n",
      "df_artists_top_10_songs_only shape: (2420, 54)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import the four dataframes\n",
    "df_artists = pd.read_csv('/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_artists.csv')\n",
    "df_songs = pd.read_csv('/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_songs.csv')\n",
    "df_albums = pd.read_csv('/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_albums.csv')\n",
    "df_artists_top_10_songs_only = pd.read_csv('/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_artists_top_10_songs_only.csv')\n",
    "\n",
    "# Display basic info about each dataframe\n",
    "print(f\"df_artists shape: {df_artists.shape}\")\n",
    "print(f\"df_songs shape: {df_songs.shape}\")\n",
    "print(f\"df_albums shape: {df_albums.shape}\")\n",
    "print(f\"df_artists_top_10_songs_only shape: {df_artists_top_10_songs_only.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc216b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available databases:\n",
      "  musicbrainz\n",
      "  musicbrainz_db\n",
      "  postgres\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "DB_PARAMS = {\n",
    "    'dbname': 'postgres',  # Connect to default postgres database\n",
    "    'user': 'musicbrainz',\n",
    "    'password': 'musicbrainz',\n",
    "    'host': 'localhost',\n",
    "    'port': 5432\n",
    "}\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(**DB_PARAMS)\n",
    "    conn.autocommit = True\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        # List all databases\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT datname \n",
    "            FROM pg_database \n",
    "            WHERE datistemplate = false\n",
    "            ORDER BY datname\n",
    "        \"\"\")\n",
    "        databases = cur.fetchall()\n",
    "        \n",
    "        print(\"Available databases:\")\n",
    "        for db in databases:\n",
    "            print(f\"  {db[0]}\")\n",
    "    \n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f91e3e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading all dataframes...\n",
      "  df_artists: (14226, 54)\n",
      "  df_songs: (5099, 16)\n",
      "  df_albums: (4569, 16)\n",
      "  df_artists_top_10_songs_only: (2420, 54)\n",
      "\n",
      "Step 2: Collecting unique artist names...\n",
      "  Found 14226 unique artist names to look up\n",
      "\n",
      "Step 3: Querying MusicBrainz database for artist IDs (batch mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 15/15 [00:27<00:00,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully matched: 11,564 artists\n",
      "  Not found in MusicBrainz: 2,662 artists\n",
      "\n",
      "Sample of artists not found (first 10):\n",
      "    - tsol\n",
      "    - the dropkick murphys\n",
      "    - auli'i cravalho\n",
      "    - beele\n",
      "    - k.w.s.\n",
      "    - danyel gerard\n",
      "    - persuasions\n",
      "    - '68\n",
      "    - 21 savage & tyler, the creator\n",
      "    - steve stevens atomic playboys\n",
      "\n",
      "Step 4: Adding musicbrainz_artist_id column to all dataframes...\n",
      "  df_artists: 11,564/14,226 rows matched (81.3%)\n",
      "  df_songs: 4,500/5,099 rows matched (88.3%)\n",
      "  df_albums: 4,181/4,569 rows matched (91.5%)\n",
      "  df_artists_top_10_songs_only: 2,041/2,420 rows matched (84.3%)\n",
      "\n",
      "Step 5: Saving updated dataframes...\n",
      "  ✓ Saved df_artists.csv\n",
      "  ✓ Saved df_songs.csv\n",
      "  ✓ Saved df_albums.csv\n",
      "  ✓ Saved df_artists_top_10_songs_only.csv\n",
      "\n",
      "================================================================================\n",
      "COMPLETE: All dataframes now have 'musicbrainz_artist_id' column\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Database connection parameters\n",
    "DB_PARAMS = {\n",
    "    'dbname': 'musicbrainz_db',\n",
    "    'user': 'musicbrainz',\n",
    "    'password': 'musicbrainz',\n",
    "    'host': 'localhost',\n",
    "    'port': 5432\n",
    "}\n",
    "\n",
    "print(\"Step 1: Loading all dataframes...\")\n",
    "df_artists = pd.read_csv('/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_artists.csv')\n",
    "df_songs = pd.read_csv('/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_songs.csv')\n",
    "df_albums = pd.read_csv('/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_albums.csv')\n",
    "df_artists_top_10_songs_only = pd.read_csv('/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_artists_top_10_songs_only.csv')\n",
    "\n",
    "print(f\"  df_artists: {df_artists.shape}\")\n",
    "print(f\"  df_songs: {df_songs.shape}\")\n",
    "print(f\"  df_albums: {df_albums.shape}\")\n",
    "print(f\"  df_artists_top_10_songs_only: {df_artists_top_10_songs_only.shape}\")\n",
    "\n",
    "# Step 2: Get all unique artist names\n",
    "print(\"\\nStep 2: Collecting unique artist names...\")\n",
    "all_artist_names = set()\n",
    "\n",
    "if 'performer_normalized' in df_artists.columns:\n",
    "    all_artist_names.update(df_artists['performer_normalized'].dropna().unique())\n",
    "\n",
    "if 'performer_normalized' in df_songs.columns:\n",
    "    all_artist_names.update(df_songs['performer_normalized'].dropna().unique())\n",
    "\n",
    "if 'performer_normalized' in df_albums.columns:\n",
    "    all_artist_names.update(df_albums['performer_normalized'].dropna().unique())\n",
    "\n",
    "if 'performer_normalized' in df_artists_top_10_songs_only.columns:\n",
    "    all_artist_names.update(df_artists_top_10_songs_only['performer_normalized'].dropna().unique())\n",
    "\n",
    "all_artist_names = list(all_artist_names)\n",
    "print(f\"  Found {len(all_artist_names)} unique artist names to look up\")\n",
    "\n",
    "# Step 3: Query MusicBrainz database for artist IDs in batches\n",
    "print(\"\\nStep 3: Querying MusicBrainz database for artist IDs (batch mode)...\")\n",
    "\n",
    "artist_name_to_id = {}\n",
    "batch_size = 1000  # Process 1000 names at a time\n",
    "\n",
    "conn = psycopg2.connect(**DB_PARAMS)\n",
    "conn.autocommit = True\n",
    "\n",
    "num_batches = (len(all_artist_names) + batch_size - 1) // batch_size\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    for i in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "        batch_names = all_artist_names[i * batch_size:(i + 1) * batch_size]\n",
    "        \n",
    "        # Single query for the entire batch using unnest and join\n",
    "        query = \"\"\"\n",
    "            SELECT DISTINCT ON (LOWER(input_name)) \n",
    "                input_name, \n",
    "                artist.id, \n",
    "                artist.name\n",
    "            FROM unnest(%s::text[]) AS input_name\n",
    "            LEFT JOIN artist ON LOWER(artist.name) = LOWER(input_name)\n",
    "            WHERE artist.id IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        cur.execute(query, (batch_names,))\n",
    "        results = cur.fetchall()\n",
    "        \n",
    "        # Map results\n",
    "        for input_name, artist_id, matched_name in results:\n",
    "            artist_name_to_id[input_name] = str(artist_id)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Find which artists weren't matched\n",
    "not_found = [name for name in all_artist_names if name not in artist_name_to_id]\n",
    "\n",
    "print(f\"  Successfully matched: {len(artist_name_to_id):,} artists\")\n",
    "print(f\"  Not found in MusicBrainz: {len(not_found):,} artists\")\n",
    "\n",
    "if not_found:\n",
    "    print(f\"\\nSample of artists not found (first 10):\")\n",
    "    for name in not_found[:10]:\n",
    "        print(f\"    - {name}\")\n",
    "\n",
    "# Step 4: Add musicbrainz_artist_id column to all dataframes\n",
    "print(\"\\nStep 4: Adding musicbrainz_artist_id column to all dataframes...\")\n",
    "\n",
    "def add_mb_id_column(df, name):\n",
    "    if 'performer_normalized' in df.columns:\n",
    "        df['musicbrainz_artist_id'] = df['performer_normalized'].map(artist_name_to_id)\n",
    "        matched = df['musicbrainz_artist_id'].notna().sum()\n",
    "        total = len(df)\n",
    "        print(f\"  {name}: {matched:,}/{total:,} rows matched ({matched/total*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"  {name}: No 'performer_normalized' column found\")\n",
    "    return df\n",
    "\n",
    "df_artists = add_mb_id_column(df_artists, 'df_artists')\n",
    "df_songs = add_mb_id_column(df_songs, 'df_songs')\n",
    "df_albums = add_mb_id_column(df_albums, 'df_albums')\n",
    "df_artists_top_10_songs_only = add_mb_id_column(df_artists_top_10_songs_only, 'df_artists_top_10_songs_only')\n",
    "\n",
    "# Step 5: Save updated dataframes\n",
    "print(\"\\nStep 5: Saving updated dataframes...\")\n",
    "df_artists.to_csv('/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_artists.csv', index=False)\n",
    "print(\"  ✓ Saved df_artists.csv\")\n",
    "\n",
    "df_songs.to_csv('/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_songs.csv', index=False)\n",
    "print(\"  ✓ Saved df_songs.csv\")\n",
    "\n",
    "df_albums.to_csv('/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_albums.csv', index=False)\n",
    "print(\"  ✓ Saved df_albums.csv\")\n",
    "\n",
    "df_artists_top_10_songs_only.to_csv('/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_artists_top_10_songs_only.csv', index=False)\n",
    "print(\"  ✓ Saved df_artists_top_10_songs_only.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLETE: All dataframes now have 'musicbrainz_artist_id' column\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd942468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Billboard artist IDs...\n",
      "Total artists in df_artists_top_10_songs_only: 2420\n",
      "Artists with MusicBrainz IDs: 2041\n",
      "Artists without MusicBrainz IDs: 379\n",
      "\n",
      "================================================================================\n",
      "PHASE 1: Extracting Master Edge List from MusicBrainz\n",
      "================================================================================\n",
      "\n",
      "Executing SQL query (this may take several minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  20%|██        | 1/5 [00:19<01:16, 19.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1/5: Found 6,175 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  40%|████      | 2/5 [00:35<00:53, 17.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 2/5: Found 4,569 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  60%|██████    | 3/5 [00:52<00:34, 17.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 3/5: Found 10,629 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  80%|████████  | 4/5 [01:07<00:16, 16.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 4/5: Found 3,298 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches: 100%|██████████| 5/5 [01:21<00:00, 16.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5/5: Found 577 edges\n",
      "\n",
      "Total edges extracted: 25,248\n",
      "Year range: 1939 - 2026\n",
      "Unique artist pairs: 15,404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Master edge list saved to: /Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/master_edge_list.parquet\n",
      "Also saved as CSV: /Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/master_edge_list.csv\n",
      "\n",
      "================================================================================\n",
      "PHASE 2: Generating Temporal Network Snapshots\n",
      "================================================================================\n",
      "\n",
      "Generating networks for years 1958 to 2024\n",
      "Total years to process: 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating yearly networks: 100%|██████████| 67/67 [00:00<00:00, 239.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PROCESSING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Summary Statistics:\n",
      "  Master edge list: 25,248 total collaboration records\n",
      "  Unique artist pairs: 15,404\n",
      "  Year range: 1939 - 2026\n",
      "  Networks generated: 67 years × 3 network types = 201 files\n",
      "\n",
      "Output locations:\n",
      "  Master edge list: /Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/master_edge_list.parquet\n",
      "  Yearly networks: /Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/networks/[YEAR]/\n",
      "    - yearly_snapshot.csv (edges from that year only)\n",
      "    - cumulative_network.csv (all edges up to that year)\n",
      "    - rolling_10year.csv (edges from previous 10 years)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Database connection parameters\n",
    "DB_PARAMS = {\n",
    "    'dbname': 'musicbrainz_db',\n",
    "    'user': 'musicbrainz',\n",
    "    'password': 'musicbrainz',\n",
    "    'host': 'localhost',\n",
    "    'port': 5432\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD BILLBOARD ARTIST IDs\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading Billboard artist IDs...\")\n",
    "df_artists_top_10 = pd.read_csv('/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_artists_top_10_songs_only.csv')\n",
    "\n",
    "# Get artist IDs and filter out NaN values, convert to integers\n",
    "artist_ids = df_artists_top_10['musicbrainz_artist_id'].dropna().astype(int).unique().tolist()\n",
    "\n",
    "print(f\"Total artists in df_artists_top_10_songs_only: {len(df_artists_top_10)}\")\n",
    "print(f\"Artists with MusicBrainz IDs: {len(artist_ids)}\")\n",
    "print(f\"Artists without MusicBrainz IDs: {df_artists_top_10['musicbrainz_artist_id'].isna().sum()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 1: Extract Master Edge List\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1: Extracting Master Edge List from MusicBrainz\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def extract_master_edge_list(artist_ids, batch_size=500):\n",
    "    \"\"\"\n",
    "    Extract all collaboration edges for the given artist IDs.\n",
    "    Uses batching to avoid memory issues with large IN clauses.\n",
    "    \"\"\"\n",
    "    \n",
    "    # SQL query template - Uses INTEGER type, not UUID\n",
    "    sql_query = \"\"\"\n",
    "    WITH EarliestRG AS (\n",
    "        -- Anchor every album/single to its original release year\n",
    "        SELECT r.release_group, MIN(rc.date_year) as first_year\n",
    "        FROM release r\n",
    "        JOIN release_country rc ON r.id = rc.release\n",
    "        WHERE rc.date_year IS NOT NULL\n",
    "        GROUP BY r.release_group\n",
    "    ),\n",
    "    CollaborationEdges AS (\n",
    "        -- Find every unique pair of artists on the same credit\n",
    "        SELECT \n",
    "            acn1.artist AS artist_a,\n",
    "            acn2.artist AS artist_b,\n",
    "            rg.id AS release_group_id,\n",
    "            erg.first_year\n",
    "        FROM artist_credit_name acn1\n",
    "        JOIN artist_credit_name acn2 ON acn1.artist_credit = acn2.artist_credit\n",
    "        JOIN release_group rg ON rg.artist_credit = acn1.artist_credit\n",
    "        JOIN EarliestRG erg ON rg.id = erg.release_group\n",
    "        WHERE acn1.artist = ANY(%s::integer[])\n",
    "          AND acn1.artist < acn2.artist\n",
    "    )\n",
    "    SELECT * FROM CollaborationEdges;\n",
    "    \"\"\"\n",
    "    \n",
    "    all_edges = []\n",
    "    \n",
    "    # Process in batches\n",
    "    num_batches = (len(artist_ids) + batch_size - 1) // batch_size\n",
    "    \n",
    "    conn = psycopg2.connect(**DB_PARAMS)\n",
    "    conn.autocommit = True\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        for i in tqdm(range(num_batches), desc=\"Extracting batches\"):\n",
    "            batch_ids = artist_ids[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            # Execute query for this batch\n",
    "            cur.execute(sql_query, (batch_ids,))\n",
    "            \n",
    "            # Fetch results\n",
    "            batch_results = cur.fetchall()\n",
    "            all_edges.extend(batch_results)\n",
    "            \n",
    "            print(f\"  Batch {i+1}/{num_batches}: Found {len(batch_results):,} edges\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_edges = pd.DataFrame(all_edges, columns=['artist_a', 'artist_b', 'release_group_id', 'first_year'])\n",
    "    \n",
    "    return df_edges\n",
    "\n",
    "# Extract the master edge list\n",
    "print(\"\\nExecuting SQL query (this may take several minutes)...\")\n",
    "master_edge_list = extract_master_edge_list(artist_ids)\n",
    "\n",
    "print(f\"\\nTotal edges extracted: {len(master_edge_list):,}\")\n",
    "print(f\"Year range: {master_edge_list['first_year'].min()} - {master_edge_list['first_year'].max()}\")\n",
    "print(f\"Unique artist pairs: {master_edge_list[['artist_a', 'artist_b']].drop_duplicates().shape[0]:,}\")\n",
    "\n",
    "# Save to parquet\n",
    "output_path = '/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/master_edge_list.parquet'\n",
    "master_edge_list.to_parquet(output_path, index=False)\n",
    "print(f\"\\nMaster edge list saved to: {output_path}\")\n",
    "\n",
    "# Also save as CSV for easy inspection\n",
    "csv_path = '/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/master_edge_list.csv'\n",
    "master_edge_list.to_csv(csv_path, index=False)\n",
    "print(f\"Also saved as CSV: {csv_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 2: Generate Temporal Network Snapshots\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2: Generating Temporal Network Snapshots\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create output directory structure\n",
    "networks_dir = Path('/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/networks')\n",
    "networks_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Determine year range\n",
    "min_year = int(master_edge_list['first_year'].min())\n",
    "max_year = int(master_edge_list['first_year'].max())\n",
    "years = range(max(1958, min_year), min(2025, max_year + 1))\n",
    "\n",
    "print(f\"\\nGenerating networks for years {max(1958, min_year)} to {min(2024, max_year)}\")\n",
    "print(f\"Total years to process: {len(list(years))}\")\n",
    "\n",
    "# Process each year\n",
    "for year in tqdm(list(years), desc=\"Generating yearly networks\"):\n",
    "    \n",
    "    # Create year directory\n",
    "    year_dir = networks_dir / str(year)\n",
    "    year_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # 1. Yearly Snapshot: Edges where first_year == year\n",
    "    yearly = master_edge_list[master_edge_list['first_year'] == year].copy()\n",
    "    yearly_dedup = yearly[['artist_a', 'artist_b']].drop_duplicates()\n",
    "    yearly_dedup.to_csv(year_dir / 'yearly_snapshot.csv', index=False)\n",
    "    \n",
    "    # 2. Cumulative Network: Edges where first_year <= year\n",
    "    cumulative = master_edge_list[master_edge_list['first_year'] <= year].copy()\n",
    "    cumulative_dedup = cumulative[['artist_a', 'artist_b']].drop_duplicates()\n",
    "    cumulative_dedup.to_csv(year_dir / 'cumulative_network.csv', index=False)\n",
    "    \n",
    "    # 3. Rolling 10-Year Network: Edges where (year - 9) <= first_year <= year\n",
    "    rolling_start = year - 9\n",
    "    rolling = master_edge_list[\n",
    "        (master_edge_list['first_year'] >= rolling_start) & \n",
    "        (master_edge_list['first_year'] <= year)\n",
    "    ].copy()\n",
    "    rolling_dedup = rolling[['artist_a', 'artist_b']].drop_duplicates()\n",
    "    rolling_dedup.to_csv(year_dir / 'rolling_10year.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"  Master edge list: {len(master_edge_list):,} total collaboration records\")\n",
    "print(f\"  Unique artist pairs: {master_edge_list[['artist_a', 'artist_b']].drop_duplicates().shape[0]:,}\")\n",
    "print(f\"  Year range: {min_year} - {max_year}\")\n",
    "print(f\"  Networks generated: {len(list(years))} years × 3 network types = {len(list(years)) * 3} files\")\n",
    "print(f\"\\nOutput locations:\")\n",
    "print(f\"  Master edge list: {output_path}\")\n",
    "print(f\"  Yearly networks: {networks_dir}/[YEAR]/\")\n",
    "print(f\"    - yearly_snapshot.csv (edges from that year only)\")\n",
    "print(f\"    - cumulative_network.csv (all edges up to that year)\")\n",
    "print(f\"    - rolling_10year.csv (edges from previous 10 years)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb403689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Billboard artist IDs (ALL ARTISTS)...\n",
      "Total artists in df_artists: 14226\n",
      "Artists with MusicBrainz IDs: 11564\n",
      "Artists without MusicBrainz IDs: 2662\n",
      "\n",
      "================================================================================\n",
      "PHASE 1: Extracting Master Edge List from MusicBrainz\n",
      "================================================================================\n",
      "\n",
      "Executing SQL query (this may take several minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:   4%|▍         | 1/24 [00:15<06:06, 15.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1/24: Found 2,518 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:   8%|▊         | 2/24 [00:29<05:19, 14.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 2/24: Found 2,467 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  12%|█▎        | 3/24 [00:44<05:08, 14.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 3/24: Found 3,277 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  17%|█▋        | 4/24 [00:58<04:50, 14.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 4/24: Found 2,861 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  21%|██        | 5/24 [01:12<04:30, 14.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5/24: Found 2,609 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  25%|██▌       | 6/24 [01:26<04:14, 14.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 6/24: Found 2,772 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  29%|██▉       | 7/24 [01:40<03:59, 14.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 7/24: Found 2,870 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  33%|███▎      | 8/24 [01:53<03:42, 13.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 8/24: Found 2,226 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  38%|███▊      | 9/24 [02:07<03:26, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 9/24: Found 2,195 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  42%|████▏     | 10/24 [02:21<03:13, 13.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/24: Found 2,202 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  46%|████▌     | 11/24 [02:36<03:03, 14.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 11/24: Found 3,074 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  50%|█████     | 12/24 [02:51<02:52, 14.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 12/24: Found 9,887 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  54%|█████▍    | 13/24 [03:05<02:38, 14.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 13/24: Found 3,465 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  58%|█████▊    | 14/24 [03:19<02:24, 14.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 14/24: Found 2,722 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  62%|██████▎   | 15/24 [03:35<02:12, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 15/24: Found 3,107 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  67%|██████▋   | 16/24 [03:48<01:55, 14.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 16/24: Found 1,904 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  71%|███████   | 17/24 [04:04<01:42, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 17/24: Found 4,001 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  75%|███████▌  | 18/24 [04:18<01:27, 14.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 18/24: Found 2,560 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  79%|███████▉  | 19/24 [04:33<01:13, 14.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 19/24: Found 1,357 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  83%|████████▎ | 20/24 [04:48<00:58, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 20/24: Found 1,134 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  88%|████████▊ | 21/24 [05:01<00:43, 14.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 21/24: Found 543 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  92%|█████████▏| 22/24 [05:15<00:28, 14.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 22/24: Found 1,848 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches:  96%|█████████▌| 23/24 [05:29<00:14, 14.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 23/24: Found 2,648 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting batches: 100%|██████████| 24/24 [05:42<00:00, 14.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 24/24: Found 1,104 edges\n",
      "\n",
      "Total edges extracted: 65,351\n",
      "Year range: 1926 - 2026\n",
      "Unique artist pairs: 42,887\n",
      "\n",
      "Master edge list saved to: /Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/master_edge_list_all_artists.parquet\n",
      "Also saved as CSV: /Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/master_edge_list_all_artists.csv\n",
      "\n",
      "================================================================================\n",
      "PHASE 2: Generating Temporal Network Snapshots\n",
      "================================================================================\n",
      "\n",
      "Generating networks for years 1958 to 2024\n",
      "Total years to process: 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating yearly networks: 100%|██████████| 67/67 [00:00<00:00, 117.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PROCESSING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Summary Statistics:\n",
      "  Master edge list: 65,351 total collaboration records\n",
      "  Unique artist pairs: 42,887\n",
      "  Year range: 1926 - 2026\n",
      "  Networks generated: 67 years × 3 network types = 201 files\n",
      "\n",
      "Output locations:\n",
      "  Master edge list: /Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/master_edge_list_all_artists.parquet\n",
      "  Yearly networks: /Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/networks_all_artists/[YEAR]/\n",
      "    - yearly_snapshot.csv (edges from that year only)\n",
      "    - cumulative_network.csv (all edges up to that year)\n",
      "    - rolling_10year.csv (edges from previous 10 years)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Database connection parameters\n",
    "DB_PARAMS = {\n",
    "    'dbname': 'musicbrainz_db',\n",
    "    'user': 'musicbrainz',\n",
    "    'password': 'musicbrainz',\n",
    "    'host': 'localhost',\n",
    "    'port': 5432\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD BILLBOARD ARTIST IDs (ALL ARTISTS)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading Billboard artist IDs (ALL ARTISTS)...\")\n",
    "df_artists = pd.read_csv('/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_artists.csv')\n",
    "\n",
    "# Get artist IDs and filter out NaN values, convert to integers\n",
    "artist_ids = df_artists['musicbrainz_artist_id'].dropna().astype(int).unique().tolist()\n",
    "\n",
    "print(f\"Total artists in df_artists: {len(df_artists)}\")\n",
    "print(f\"Artists with MusicBrainz IDs: {len(artist_ids)}\")\n",
    "print(f\"Artists without MusicBrainz IDs: {df_artists['musicbrainz_artist_id'].isna().sum()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 1: Extract Master Edge List\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1: Extracting Master Edge List from MusicBrainz\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def extract_master_edge_list(artist_ids, batch_size=500):\n",
    "    \"\"\"\n",
    "    Extract all collaboration edges for the given artist IDs.\n",
    "    Uses batching to avoid memory issues with large IN clauses.\n",
    "    \"\"\"\n",
    "    \n",
    "    # SQL query template - Uses INTEGER type, not UUID\n",
    "    sql_query = \"\"\"\n",
    "    WITH EarliestRG AS (\n",
    "        -- Anchor every album/single to its original release year\n",
    "        SELECT r.release_group, MIN(rc.date_year) as first_year\n",
    "        FROM release r\n",
    "        JOIN release_country rc ON r.id = rc.release\n",
    "        WHERE rc.date_year IS NOT NULL\n",
    "        GROUP BY r.release_group\n",
    "    ),\n",
    "    CollaborationEdges AS (\n",
    "        -- Find every unique pair of artists on the same credit\n",
    "        SELECT \n",
    "            acn1.artist AS artist_a,\n",
    "            acn2.artist AS artist_b,\n",
    "            rg.id AS release_group_id,\n",
    "            erg.first_year\n",
    "        FROM artist_credit_name acn1\n",
    "        JOIN artist_credit_name acn2 ON acn1.artist_credit = acn2.artist_credit\n",
    "        JOIN release_group rg ON rg.artist_credit = acn1.artist_credit\n",
    "        JOIN EarliestRG erg ON rg.id = erg.release_group\n",
    "        WHERE acn1.artist = ANY(%s::integer[])\n",
    "          AND acn1.artist < acn2.artist\n",
    "    )\n",
    "    SELECT * FROM CollaborationEdges;\n",
    "    \"\"\"\n",
    "    \n",
    "    all_edges = []\n",
    "    \n",
    "    # Process in batches\n",
    "    num_batches = (len(artist_ids) + batch_size - 1) // batch_size\n",
    "    \n",
    "    conn = psycopg2.connect(**DB_PARAMS)\n",
    "    conn.autocommit = True\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        for i in tqdm(range(num_batches), desc=\"Extracting batches\"):\n",
    "            batch_ids = artist_ids[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            # Execute query for this batch\n",
    "            cur.execute(sql_query, (batch_ids,))\n",
    "            \n",
    "            # Fetch results\n",
    "            batch_results = cur.fetchall()\n",
    "            all_edges.extend(batch_results)\n",
    "            \n",
    "            print(f\"  Batch {i+1}/{num_batches}: Found {len(batch_results):,} edges\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_edges = pd.DataFrame(all_edges, columns=['artist_a', 'artist_b', 'release_group_id', 'first_year'])\n",
    "    \n",
    "    return df_edges\n",
    "\n",
    "# Extract the master edge list\n",
    "print(\"\\nExecuting SQL query (this may take several minutes)...\")\n",
    "master_edge_list = extract_master_edge_list(artist_ids)\n",
    "\n",
    "print(f\"\\nTotal edges extracted: {len(master_edge_list):,}\")\n",
    "print(f\"Year range: {master_edge_list['first_year'].min()} - {master_edge_list['first_year'].max()}\")\n",
    "print(f\"Unique artist pairs: {master_edge_list[['artist_a', 'artist_b']].drop_duplicates().shape[0]:,}\")\n",
    "\n",
    "# Save to parquet\n",
    "output_path = '/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/master_edge_list_all_artists.parquet'\n",
    "master_edge_list.to_parquet(output_path, index=False)\n",
    "print(f\"\\nMaster edge list saved to: {output_path}\")\n",
    "\n",
    "# Also save as CSV for easy inspection\n",
    "csv_path = '/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/master_edge_list_all_artists.csv'\n",
    "master_edge_list.to_csv(csv_path, index=False)\n",
    "print(f\"Also saved as CSV: {csv_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 2: Generate Temporal Network Snapshots\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2: Generating Temporal Network Snapshots\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create output directory structure\n",
    "networks_dir = Path('/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/networks_all_artists')\n",
    "networks_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Determine year range\n",
    "min_year = int(master_edge_list['first_year'].min())\n",
    "max_year = int(master_edge_list['first_year'].max())\n",
    "years = range(max(1958, min_year), min(2025, max_year + 1))\n",
    "\n",
    "print(f\"\\nGenerating networks for years {max(1958, min_year)} to {min(2024, max_year)}\")\n",
    "print(f\"Total years to process: {len(list(years))}\")\n",
    "\n",
    "# Process each year\n",
    "for year in tqdm(list(years), desc=\"Generating yearly networks\"):\n",
    "    \n",
    "    # Create year directory\n",
    "    year_dir = networks_dir / str(year)\n",
    "    year_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # 1. Yearly Snapshot: Edges where first_year == year\n",
    "    yearly = master_edge_list[master_edge_list['first_year'] == year].copy()\n",
    "    yearly_dedup = yearly[['artist_a', 'artist_b']].drop_duplicates()\n",
    "    yearly_dedup.to_csv(year_dir / 'yearly_snapshot.csv', index=False)\n",
    "    \n",
    "    # 2. Cumulative Network: Edges where first_year <= year\n",
    "    cumulative = master_edge_list[master_edge_list['first_year'] <= year].copy()\n",
    "    cumulative_dedup = cumulative[['artist_a', 'artist_b']].drop_duplicates()\n",
    "    cumulative_dedup.to_csv(year_dir / 'cumulative_network.csv', index=False)\n",
    "    \n",
    "    # 3. Rolling 10-Year Network: Edges where (year - 9) <= first_year <= year\n",
    "    rolling_start = year - 9\n",
    "    rolling = master_edge_list[\n",
    "        (master_edge_list['first_year'] >= rolling_start) & \n",
    "        (master_edge_list['first_year'] <= year)\n",
    "    ].copy()\n",
    "    rolling_dedup = rolling[['artist_a', 'artist_b']].drop_duplicates()\n",
    "    rolling_dedup.to_csv(year_dir / 'rolling_10year.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"  Master edge list: {len(master_edge_list):,} total collaboration records\")\n",
    "print(f\"  Unique artist pairs: {master_edge_list[['artist_a', 'artist_b']].drop_duplicates().shape[0]:,}\")\n",
    "print(f\"  Year range: {min_year} - {max_year}\")\n",
    "print(f\"  Networks generated: {len(list(years))} years × 3 network types = {len(list(years)) * 3} files\")\n",
    "print(f\"\\nOutput locations:\")\n",
    "print(f\"  Master edge list: {output_path}\")\n",
    "print(f\"  Yearly networks: {networks_dir}/[YEAR]/\")\n",
    "print(f\"    - yearly_snapshot.csv (edges from that year only)\")\n",
    "print(f\"    - cumulative_network.csv (all edges up to that year)\")\n",
    "print(f\"    - rolling_10year.csv (edges from previous 10 years)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da42acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading df_artists...\n",
      "Building year-to-artist index maps...\n",
      "Found 69 unique years to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:   0%|          | 0/69 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Year 1958\n",
      "  yearly: Computing metrics... (Nodes: 197, Edges: 152) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 691, Edges: 737) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 873, Edges: 1,034)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:   1%|▏         | 1/69 [00:00<00:42,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1959\n",
      "  yearly: Computing metrics... (Nodes: 181, Edges: 135) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 759, Edges: 813) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 960, Edges: 1,136)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:   3%|▎         | 2/69 [00:01<00:44,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1960\n",
      "  yearly: Computing metrics... (Nodes: 162, Edges: 110) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 812, Edges: 846) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 1,045, Edges: 1,221)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:   4%|▍         | 3/69 [00:02<00:49,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1961\n",
      "  yearly: Computing metrics... (Nodes: 173, Edges: 126) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 857, Edges: 879) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 1,136, Edges: 1,318)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:   6%|▌         | 4/69 [00:03<00:50,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1962\n",
      "  yearly: Computing metrics... (Nodes: 212, Edges: 149) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 937, Edges: 955) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 1,248, Edges: 1,437)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:   7%|▋         | 5/69 [00:03<00:54,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1963\n",
      "  yearly: Computing metrics... (Nodes: 169, Edges: 127) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 984, Edges: 1,007) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 1,322, Edges: 1,525)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:   9%|▊         | 6/69 [00:04<00:56,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1964\n",
      "  yearly: Computing metrics... (Nodes: 170, Edges: 119) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,042, Edges: 1,032) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 1,415, Edges: 1,611)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  10%|█         | 7/69 [00:05<00:58,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1965\n",
      "  yearly: Computing metrics... (Nodes: 227, Edges: 182) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,100, Edges: 1,101) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 1,520, Edges: 1,746)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  12%|█▏        | 8/69 [00:07<01:02,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1966\n",
      "  yearly: Computing metrics... (Nodes: 191, Edges: 137) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,132, Edges: 1,106) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 1,621, Edges: 1,848)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  13%|█▎        | 9/69 [00:08<01:07,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1967\n",
      "  yearly: Computing metrics... (Nodes: 159, Edges: 114) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,138, Edges: 1,097) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 1,693, Edges: 1,935)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  14%|█▍        | 10/69 [00:09<01:09,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1968\n",
      "  yearly: Computing metrics... (Nodes: 221, Edges: 154) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,151, Edges: 1,102) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 1,806, Edges: 2,058)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  16%|█▌        | 11/69 [00:11<01:13,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1969\n",
      "  yearly: Computing metrics... (Nodes: 179, Edges: 146) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,134, Edges: 1,103) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 1,875, Edges: 2,156)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  17%|█▋        | 12/69 [00:12<01:18,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1970\n",
      "  yearly: Computing metrics... (Nodes: 240, Edges: 190) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,174, Edges: 1,169) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 1,988, Edges: 2,292)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  19%|█▉        | 13/69 [00:14<01:26,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1971\n",
      "  yearly: Computing metrics... (Nodes: 218, Edges: 185) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,196, Edges: 1,210) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 2,084, Edges: 2,425)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  20%|██        | 14/69 [00:16<01:32,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1972\n",
      "  yearly: Computing metrics... (Nodes: 298, Edges: 264) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,259, Edges: 1,308) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 2,240, Edges: 2,626)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  22%|██▏       | 15/69 [00:19<01:42,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1973\n",
      "  yearly: Computing metrics... (Nodes: 294, Edges: 261) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,336, Edges: 1,408) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 2,383, Edges: 2,818)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  23%|██▎       | 16/69 [00:21<01:51,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1974\n",
      "  yearly: Computing metrics... (Nodes: 270, Edges: 231) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,420, Edges: 1,513) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 2,527, Edges: 3,005)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  25%|██▍       | 17/69 [00:24<02:01,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1975\n",
      "  yearly: Computing metrics... (Nodes: 207, Edges: 187) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,407, Edges: 1,500) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 2,608, Edges: 3,126)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  26%|██▌       | 18/69 [00:27<02:09,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1976\n",
      "  yearly: Computing metrics... (Nodes: 252, Edges: 192) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,448, Edges: 1,557) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 2,718, Edges: 3,263)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  28%|██▊       | 19/69 [00:31<02:17,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1977\n",
      "  yearly: Computing metrics... (Nodes: 270, Edges: 196) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,532, Edges: 1,637) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 2,855, Edges: 3,415)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  29%|██▉       | 20/69 [00:34<02:26,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1978\n",
      "  yearly: Computing metrics... (Nodes: 257, Edges: 209) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,553, Edges: 1,672) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 2,967, Edges: 3,556)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  30%|███       | 21/69 [00:38<02:38,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1979\n",
      "  yearly: Computing metrics... (Nodes: 277, Edges: 249) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,617, Edges: 1,761) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 3,083, Edges: 3,737)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  32%|███▏      | 22/69 [00:42<02:48,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1980\n",
      "  yearly: Computing metrics... (Nodes: 290, Edges: 259) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,670, Edges: 1,833) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 3,218, Edges: 3,930)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  33%|███▎      | 23/69 [00:47<02:59,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1981\n",
      "  yearly: Computing metrics... (Nodes: 331, Edges: 273) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,737, Edges: 1,917) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 3,354, Edges: 4,131)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  35%|███▍      | 24/69 [00:52<03:09,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1982\n",
      "  yearly: Computing metrics... (Nodes: 340, Edges: 290) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,763, Edges: 1,951) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 3,503, Edges: 4,348)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  36%|███▌      | 25/69 [00:58<03:26,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1983\n",
      "  yearly: Computing metrics... (Nodes: 309, Edges: 236) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,770, Edges: 1,932) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 3,617, Edges: 4,503)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  38%|███▊      | 26/69 [01:04<03:36,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1984\n",
      "  yearly: Computing metrics... (Nodes: 334, Edges: 273) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,820, Edges: 1,959) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 3,765, Edges: 4,706)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  39%|███▉      | 27/69 [01:10<03:48,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1985\n",
      "  yearly: Computing metrics... (Nodes: 406, Edges: 330) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 1,944, Edges: 2,069) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 3,945, Edges: 4,939)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  41%|████      | 28/69 [01:17<04:02,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1986\n",
      "  yearly: Computing metrics... (Nodes: 429, Edges: 369) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 2,057, Edges: 2,201) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 4,126, Edges: 5,189)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  42%|████▏     | 29/69 [01:25<04:17,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1987\n",
      "  yearly: Computing metrics... (Nodes: 517, Edges: 522) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 2,153, Edges: 2,420) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 4,336, Edges: 5,529)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  43%|████▎     | 30/69 [01:33<04:37,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1988\n",
      "  yearly: Computing metrics... (Nodes: 564, Edges: 522) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 2,339, Edges: 2,652) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 4,570, Edges: 5,863)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  45%|████▍     | 31/69 [01:43<05:03,  7.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1989\n",
      "  yearly: Computing metrics... (Nodes: 595, Edges: 508) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 2,486, Edges: 2,820) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 4,786, Edges: 6,174)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  46%|████▋     | 32/69 [01:54<05:28,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1990\n",
      "  yearly: Computing metrics... (Nodes: 659, Edges: 592) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 2,690, Edges: 3,054) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 5,059, Edges: 6,565)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  48%|████▊     | 33/69 [02:06<05:55,  9.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1991\n",
      "  yearly: Computing metrics... (Nodes: 663, Edges: 625) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 2,859, Edges: 3,280) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 5,303, Edges: 6,947)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  49%|████▉     | 34/69 [02:20<06:21, 10.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1992\n",
      "  yearly: Computing metrics... (Nodes: 768, Edges: 708) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 3,066, Edges: 3,580) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 5,604, Edges: 7,399)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  51%|█████     | 35/69 [02:34<06:48, 12.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1993\n",
      "  yearly: Computing metrics... (Nodes: 712, Edges: 668) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 3,277, Edges: 3,874) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 5,861, Edges: 7,796)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  52%|█████▏    | 36/69 [02:50<07:14, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1994\n",
      "  yearly: Computing metrics... (Nodes: 793, Edges: 720) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 3,537, Edges: 4,209) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 6,183, Edges: 8,253)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  54%|█████▎    | 37/69 [03:08<07:41, 14.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1995\n",
      "  yearly: Computing metrics... (Nodes: 747, Edges: 644) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 3,739, Edges: 4,444) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 6,491, Edges: 8,671)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  55%|█████▌    | 38/69 [03:27<08:12, 15.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1996\n",
      "  yearly: Computing metrics... (Nodes: 773, Edges: 716) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 3,921, Edges: 4,711) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 6,798, Edges: 9,138)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  57%|█████▋    | 39/69 [03:49<08:49, 17.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1997\n",
      "  yearly: Computing metrics... (Nodes: 776, Edges: 682) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 4,113, Edges: 4,868) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 7,125, Edges: 9,585)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  58%|█████▊    | 40/69 [04:12<09:18, 19.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1998\n",
      "  yearly: Computing metrics... (Nodes: 804, Edges: 672) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 4,316, Edges: 5,045) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 7,453, Edges: 10,020)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  59%|█████▉    | 41/69 [04:38<09:57, 21.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 1999\n",
      "  yearly: Computing metrics... (Nodes: 876, Edges: 1,254) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 4,551, Edges: 5,813) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 7,845, Edges: 11,043)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  61%|██████    | 42/69 [05:07<10:40, 23.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2000\n",
      "  yearly: Computing metrics... (Nodes: 820, Edges: 700) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 4,684, Edges: 5,952) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 8,199, Edges: 11,510)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  62%|██████▏   | 43/69 [05:39<11:16, 26.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2001\n",
      "  yearly: Computing metrics... (Nodes: 778, Edges: 684) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 4,786, Edges: 6,024) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 8,513, Edges: 11,925)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  64%|██████▍   | 44/69 [06:12<11:46, 28.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2002\n",
      "  yearly: Computing metrics... (Nodes: 826, Edges: 641) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 4,869, Edges: 6,028) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 8,861, Edges: 12,356)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  65%|██████▌   | 45/69 [06:48<12:10, 30.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2003\n",
      "  yearly: Computing metrics... (Nodes: 816, Edges: 723) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 4,979, Edges: 6,167) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 9,194, Edges: 12,852)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  67%|██████▋   | 46/69 [07:26<12:35, 32.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2004\n",
      "  yearly: Computing metrics... (Nodes: 797, Edges: 642) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 5,019, Edges: 6,163) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 9,526, Edges: 13,304)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  68%|██████▊   | 47/69 [08:07<12:53, 35.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2005\n",
      "  yearly: Computing metrics... (Nodes: 933, Edges: 821) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 5,152, Edges: 6,340) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 9,914, Edges: 13,899)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  70%|██████▉   | 48/69 [08:51<13:17, 37.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2006\n",
      "  yearly: Computing metrics... (Nodes: 1,014, Edges: 871) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 5,304, Edges: 6,490) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 10,335, Edges: 14,531)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  71%|███████   | 49/69 [09:39<13:41, 41.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2007\n",
      "  yearly: Computing metrics... (Nodes: 956, Edges: 820) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 5,466, Edges: 6,661) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 10,744, Edges: 15,120)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  72%|███████▏  | 50/69 [10:31<14:01, 44.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2008\n",
      "  yearly: Computing metrics... (Nodes: 1,025, Edges: 806) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 5,607, Edges: 6,826) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 11,186, Edges: 15,724)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  74%|███████▍  | 51/69 [11:26<14:11, 47.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2009\n",
      "  yearly: Computing metrics... (Nodes: 1,019, Edges: 854) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 5,702, Edges: 6,461) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 11,658, Edges: 16,397)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  75%|███████▌  | 52/69 [12:25<14:23, 50.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2010\n",
      "  yearly: Computing metrics... (Nodes: 1,139, Edges: 1,048) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 5,891, Edges: 6,815) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 12,174, Edges: 17,251)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  77%|███████▋  | 53/69 [13:31<14:48, 55.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2011\n",
      "  yearly: Computing metrics... (Nodes: 1,234, Edges: 1,074) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 6,144, Edges: 7,192) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 12,720, Edges: 18,084)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  78%|███████▊  | 54/69 [26:20<1:07:23, 269.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2012\n",
      "  yearly: Computing metrics... (Nodes: 1,400, Edges: 1,322) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 6,489, Edges: 7,819) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 13,356, Edges: 19,124)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  80%|███████▉  | 55/69 [27:40<49:38, 212.74s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2013\n",
      "  yearly: Computing metrics... (Nodes: 1,462, Edges: 1,284) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 6,925, Edges: 8,338) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 14,067, Edges: 20,158)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  81%|████████  | 56/69 [29:09<38:03, 175.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2014\n",
      "  yearly: Computing metrics... (Nodes: 1,574, Edges: 1,396) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 7,374, Edges: 8,999) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 14,793, Edges: 21,273)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  83%|████████▎ | 57/69 [30:49<30:35, 152.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2015\n",
      "  yearly: Computing metrics... (Nodes: 1,694, Edges: 1,650) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 7,789, Edges: 9,736) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 15,531, Edges: 22,598)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  84%|████████▍ | 58/69 [32:45<25:59, 141.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2016\n",
      "  yearly: Computing metrics... (Nodes: 1,833, Edges: 1,927) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 8,235, Edges: 10,686) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 16,378, Edges: 24,180)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  86%|████████▌ | 59/69 [34:57<23:08, 138.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2017\n",
      "  yearly: Computing metrics... (Nodes: 1,995, Edges: 2,232) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 8,752, Edges: 11,863) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 17,287, Edges: 25,979)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  87%|████████▋ | 60/69 [37:29<21:25, 142.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2018\n",
      "  yearly: Computing metrics... (Nodes: 2,241, Edges: 2,391) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 9,373, Edges: 13,176) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 18,283, Edges: 27,880)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  88%|████████▊ | 61/69 [59:57<1:07:15, 504.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2019\n",
      "  yearly: Computing metrics... (Nodes: 2,435, Edges: 2,584) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 10,091, Edges: 14,641) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 19,368, Edges: 29,974)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  90%|████████▉ | 62/69 [1:07:14<56:29, 484.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2020\n",
      "  yearly: Computing metrics... (Nodes: 2,814, Edges: 3,603) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 10,989, Edges: 16,952) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 20,626, Edges: 33,069)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  91%|█████████▏| 63/69 [1:11:23<41:21, 413.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2021\n",
      "  yearly: Computing metrics... (Nodes: 2,939, Edges: 3,186) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 11,901, Edges: 18,828) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 21,909, Edges: 35,729)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  93%|█████████▎| 64/69 [1:16:43<32:07, 385.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2022\n",
      "  yearly: Computing metrics... (Nodes: 2,514, Edges: 2,300) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 12,377, Edges: 19,665) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 22,917, Edges: 37,597)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  94%|█████████▍| 65/69 [1:32:13<36:34, 548.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2023\n",
      "  yearly: Computing metrics... (Nodes: 2,492, Edges: 2,287) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 12,841, Edges: 20,531) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 23,929, Edges: 39,467)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years:  96%|█████████▌| 66/69 [1:38:28<24:50, 496.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2024\n",
      "  yearly: Computing metrics... (Nodes: 2,393, Edges: 2,113) ✓\n",
      "  rolling10: Computing metrics... (Nodes: 13,186, Edges: 21,200) ✓\n",
      "  cumulative: Computing metrics... (Nodes: 24,858, Edges: 41,190)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing years: 100%|██████████| 69/69 [1:45:34<00:00, 91.81s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓\n",
      "\n",
      "================================================================================\n",
      "Year 2025\n",
      "  yearly: File not found, skipping\n",
      "  rolling10: File not found, skipping\n",
      "  cumulative: File not found, skipping\n",
      "\n",
      "================================================================================\n",
      "Year 2026\n",
      "  yearly: File not found, skipping\n",
      "  rolling10: File not found, skipping\n",
      "  cumulative: File not found, skipping\n",
      "\n",
      "================================================================================\n",
      "Saving results to /Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_artists_with_network_metrics.csv...\n",
      "✓ Complete!\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Metrics calculated for 69 unique years\n",
      "Total columns in output: 85\n",
      "\n",
      "Output saved to: /Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_artists_with_network_metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "NETWORKS_DIR = Path('/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/networks_all_artists')\n",
    "DF_ARTISTS_PATH = '/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_artists.csv'\n",
    "OUTPUT_PATH = '/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_artists_with_network_metrics.csv'\n",
    "\n",
    "# ============================================================================\n",
    "# DATA PREPARATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading df_artists...\")\n",
    "df_artists = pd.read_csv(DF_ARTISTS_PATH)\n",
    "\n",
    "# Keep IDs as integers (matching the network edge lists)\n",
    "df_artists['musicbrainz_artist_id'] = df_artists['musicbrainz_artist_id'].fillna(-1).astype(int)\n",
    "\n",
    "# Initialize all metric columns with None\n",
    "metric_names = ['degree', 'closeness', 'harmonic_closeness', 'betweenness', 'eigenvector']\n",
    "network_types_short = ['yearly', 'rolling10', 'cumulative']\n",
    "\n",
    "for metric in metric_names:\n",
    "    for net_type in network_types_short:\n",
    "        df_artists[f'{metric}_centrality_top10_{net_type}'] = None\n",
    "        df_artists[f'{metric}_centrality_firstsong_{net_type}'] = None\n",
    "\n",
    "# Map years to lists of artist indices\n",
    "print(\"Building year-to-artist index maps...\")\n",
    "top10_year_map = {}\n",
    "for idx, row in df_artists.iterrows():\n",
    "    year = row['first_year_top_10_songs']\n",
    "    if pd.notna(year):\n",
    "        year = int(year)\n",
    "        if year not in top10_year_map:\n",
    "            top10_year_map[year] = []\n",
    "        top10_year_map[year].append(idx)\n",
    "\n",
    "firstsong_year_map = {}\n",
    "for idx, row in df_artists.iterrows():\n",
    "    year = row['first_song_year']\n",
    "    if pd.notna(year):\n",
    "        year = int(year)\n",
    "        if year not in firstsong_year_map:\n",
    "            firstsong_year_map[year] = []\n",
    "        firstsong_year_map[year].append(idx)\n",
    "\n",
    "# Get unique sorted years to process\n",
    "all_relevant_years = sorted(set(top10_year_map.keys()) | set(firstsong_year_map.keys()))\n",
    "print(f\"Found {len(all_relevant_years)} unique years to process\")\n",
    "\n",
    "# ============================================================================\n",
    "# CENTRALITY COMPUTATION ENGINE\n",
    "# ============================================================================\n",
    "\n",
    "def get_all_metrics(G):\n",
    "    \"\"\"Calculates all 5 centralities for ALL nodes in G at once.\"\"\"\n",
    "    if G is None or len(G) == 0:\n",
    "        return {m: {} for m in metric_names}\n",
    "    \n",
    "    print(f\" (Nodes: {len(G):,}, Edges: {len(G.edges()):,})\", end=\"\")\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    try:\n",
    "        metrics['degree'] = nx.degree_centrality(G)\n",
    "    except:\n",
    "        metrics['degree'] = {}\n",
    "    \n",
    "    try:\n",
    "        metrics['closeness'] = nx.closeness_centrality(G)\n",
    "    except:\n",
    "        metrics['closeness'] = {}\n",
    "    \n",
    "    try:\n",
    "        metrics['harmonic_closeness'] = nx.harmonic_centrality(G)\n",
    "    except:\n",
    "        metrics['harmonic_closeness'] = {}\n",
    "    \n",
    "    try:\n",
    "        # Sample for large graphs to speed up betweenness\n",
    "        k = min(len(G), 500) if len(G) > 500 else None\n",
    "        metrics['betweenness'] = nx.betweenness_centrality(G, k=k)\n",
    "    except:\n",
    "        metrics['betweenness'] = {}\n",
    "    \n",
    "    try:\n",
    "        metrics['eigenvector'] = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "    except:\n",
    "        metrics['eigenvector'] = {}\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN LOOP (By Year)\n",
    "# ============================================================================\n",
    "\n",
    "network_types = {\n",
    "    'yearly': 'yearly_snapshot',\n",
    "    'rolling10': 'rolling_10year',\n",
    "    'cumulative': 'cumulative_network'\n",
    "}\n",
    "\n",
    "for year in tqdm(all_relevant_years, desc=\"Processing years\"):\n",
    "    print(f\"\\n{'='*80}\\nYear {year}\")\n",
    "    \n",
    "    for net_key, net_filename in network_types.items():\n",
    "        \n",
    "        # 1. Load the network file once\n",
    "        file_path = NETWORKS_DIR / str(year) / f'{net_filename}.csv'\n",
    "        if not file_path.exists():\n",
    "            print(f\"  {net_key}: File not found, skipping\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            edges = pd.read_csv(file_path)\n",
    "            if len(edges) == 0:\n",
    "                print(f\"  {net_key}: Empty network, skipping\")\n",
    "                continue\n",
    "                \n",
    "            G = nx.from_pandas_edgelist(edges, 'artist_a', 'artist_b')\n",
    "            \n",
    "            # 2. Compute metrics for every node in this year's graph\n",
    "            print(f\"  {net_key}: Computing metrics...\", end=\"\")\n",
    "            all_node_metrics = get_all_metrics(G)\n",
    "            print(\" ✓\")\n",
    "            \n",
    "            # 3. Update all artists who had their first Top 10 hit in this year\n",
    "            if year in top10_year_map:\n",
    "                for idx in top10_year_map[year]:\n",
    "                    m_id = int(df_artists.at[idx, 'musicbrainz_artist_id'])\n",
    "                    if m_id == -1:  # Skip artists without MusicBrainz ID\n",
    "                        continue\n",
    "                    \n",
    "                    for m_name in metric_names:\n",
    "                        val = all_node_metrics[m_name].get(m_id)\n",
    "                        df_artists.at[idx, f'{m_name}_centrality_top10_{net_key}'] = val\n",
    "\n",
    "            # 4. Update all artists who had their first charting song in this year\n",
    "            if year in firstsong_year_map:\n",
    "                for idx in firstsong_year_map[year]:\n",
    "                    m_id = int(df_artists.at[idx, 'musicbrainz_artist_id'])\n",
    "                    if m_id == -1:  # Skip artists without MusicBrainz ID\n",
    "                        continue\n",
    "                    \n",
    "                    for m_name in metric_names:\n",
    "                        val = all_node_metrics[m_name].get(m_id)\n",
    "                        df_artists.at[idx, f'{m_name}_centrality_firstsong_{net_key}'] = val\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  {net_key}: Error - {e}\")\n",
    "            continue\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Saving results to {OUTPUT_PATH}...\")\n",
    "df_artists.to_csv(OUTPUT_PATH, index=False)\n",
    "print(\"✓ Complete!\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nMetrics calculated for {len(all_relevant_years)} unique years\")\n",
    "print(f\"Total columns in output: {len(df_artists.columns)}\")\n",
    "print(f\"\\nOutput saved to: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82456ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataframes...\n",
      "df_artists_with_metrics: (14226, 85)\n",
      "df_artists_top_10_songs_only: (2420, 55)\n",
      "\n",
      "Found 30 network metric columns to transfer\n",
      "\n",
      "Merging dataframes on 'performer_normalized'...\n",
      "Result shape: (2420, 85)\n",
      "\n",
      "Metrics added for 407/2420 artists (16.8%)\n",
      "\n",
      "Saving to /Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_artists_top_10_songs_only_with_network_metrics.csv...\n",
      "✓ Complete!\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Original df_artists_top_10_songs_only columns: 55\n",
      "Network metric columns added: 30\n",
      "New total columns: 85\n",
      "\n",
      "Output file: df_artists_top_10_songs_only_with_network_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading dataframes...\")\n",
    "df_artists_with_metrics = pd.read_csv('/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_artists_with_network_metrics.csv')\n",
    "df_artists_top_10 = pd.read_csv('/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_artists_top_10_songs_only.csv')\n",
    "\n",
    "print(f\"df_artists_with_metrics: {df_artists_with_metrics.shape}\")\n",
    "print(f\"df_artists_top_10_songs_only: {df_artists_top_10.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# IDENTIFY NETWORK METRIC COLUMNS\n",
    "# ============================================================================\n",
    "\n",
    "# Get all the network metric column names\n",
    "network_columns = [col for col in df_artists_with_metrics.columns \n",
    "                   if '_centrality_' in col]\n",
    "\n",
    "print(f\"\\nFound {len(network_columns)} network metric columns to transfer\")\n",
    "\n",
    "# ============================================================================\n",
    "# MERGE DATAFRAMES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nMerging dataframes on 'performer_normalized'...\")\n",
    "\n",
    "# Select only the columns we need from df_artists_with_metrics\n",
    "columns_to_merge = ['performer_normalized'] + network_columns\n",
    "df_metrics_subset = df_artists_with_metrics[columns_to_merge]\n",
    "\n",
    "# Merge with df_artists_top_10\n",
    "df_result = df_artists_top_10.merge(\n",
    "    df_metrics_subset,\n",
    "    on='performer_normalized',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Result shape: {df_result.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFY MERGE\n",
    "# ============================================================================\n",
    "\n",
    "# Check how many artists got matched\n",
    "matched = df_result[network_columns[0]].notna().sum()\n",
    "total = len(df_result)\n",
    "print(f\"\\nMetrics added for {matched}/{total} artists ({matched/total*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "output_path = '/Users/jamesemcnally/Documents/GitHub/spring-2026-hitmakers/df_artists_top_10_songs_only_with_network_metrics.csv'\n",
    "print(f\"\\nSaving to {output_path}...\")\n",
    "df_result.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"✓ Complete!\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Original df_artists_top_10_songs_only columns: {len(df_artists_top_10.columns)}\")\n",
    "print(f\"Network metric columns added: {len(network_columns)}\")\n",
    "print(f\"New total columns: {len(df_result.columns)}\")\n",
    "print(f\"\\nOutput file: df_artists_top_10_songs_only_with_network_metrics.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
